{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/mltensor.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1252868\n"
     ]
    }
   ],
   "source": [
    "start_indx = text.find('Python Machine Learning')\n",
    "end_indx = text.find('[ 741 ]')\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "\n",
    "print('Total Length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters: 272\n"
     ]
    }
   ],
   "source": [
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i, ch in enumerate(chars_sorted) }\n",
    "char_array = np.array(chars_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1252868,)\n"
     ]
    }
   ],
   "source": [
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print(\"Text encoded shape: \", text_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Machine  === Encoded ==>  [50 90 85 73 80 79  2 47 66 68 73 74 79 70  2]\n"
     ]
    }
   ],
   "source": [
    "print(text[:15], '=== Encoded ==> ', text_encoded[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46 70 66 83 79 74 79 72  1 54 73 74 83 69  2 39 69 74 85 74] === reverse ==> Learning\n",
      "Third Editi\n"
     ]
    }
   ],
   "source": [
    "print(text_encoded[15:35], '=== reverse ==>', ''.join(char_array[text_encoded[15:35]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 --> P\n",
      "90 --> y\n",
      "85 --> t\n",
      "73 --> h\n",
      "80 --> o\n"
     ]
    }
   ],
   "source": [
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for example in ds_text_encoded.take(5):\n",
    "    print('{} --> {}'.format(example.numpy(), char_array[example.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "chunk_size = seq_len + 1\n",
    "\n",
    "ds_chunks = ds_text_encoded.batch(batch_size=chunk_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    \n",
    "    return input_seq, target_seq\n",
    "\n",
    "ds_sequences = ds_chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input (x) :  'Python Machine Learning\\nThird Edition\\nMachine Lear'\n",
      "  Target (y)  :  'ython Machine Learning\\nThird Edition\\nMachine Learn'\n",
      "\n",
      "  Input (x) :  'ing and Deep Learning with Python,\\nscikit-learn, a'\n",
      "  Target (y)  :  'ng and Deep Learning with Python,\\nscikit-learn, an'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in ds_sequences.take(2):\n",
    "    print('  Input (x) : ', repr(''.join(char_array[sample[0].numpy()])))\n",
    "    print('  Target (y)  : ', repr(''.join(char_array[sample[1].numpy()])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a character-level RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set training parameters.\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         69632     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 512)         1574912   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 272)         139536    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1784080 (6.81 MB)\n",
      "Trainable params: 1784080 (6.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = build_model(vocab_size=charset_size, \n",
    "                    embedding_dim=embedding_dim, \n",
    "                    rnn_units=rnn_units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "384/384 [==============================] - 1078s 3s/step - loss: 2.8414\n",
      "Epoch 2/20\n",
      "216/384 [===============>..............] - ETA: 7:53 - loss: 2.2502"
     ]
    }
   ],
   "source": [
    "model.fit(ds, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation phase â€“ generating new text passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "logits = [[1.0, 1.0, 1.0]]\n",
    "print('Probabilities: ', tf.math.softmax(logits).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tf.random.categorical(logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str,\n",
    "           len_generated_txt=700, \n",
    "           max_input_length=50, scale_factor=1.0):\n",
    "    encoded_input = [char2int[s] for s in starting string]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "    \n",
    "    generated_str = starting_str\n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_txt):\n",
    "        logits = model(encoded_input)\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "        \n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_idx = tf.random.categorical(scaled_logits, num_samples=1)\n",
    "        new_char_idx = tf.squeeze(new_char_idx)[0].numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
