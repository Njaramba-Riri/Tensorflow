# -*- coding: utf-8 -*-
"""langModelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BWc5zD9oig3r8XxMlgDoeLaGStcWo5I7
"""

import numpy as np

from google.colab import drive
import tensorflow as tf

drive.mount('/content/drive')

with open('sample_data/island.txt', 'r') as f:
    text = f.read()

start_indx = text.find('THE MYSTERIOUS ISLAND')
end_indx = text.find('End of the Project Gutenberg')
text = text[start_indx:end_indx]
char_set = set(text)

print('Total Length:', len(text))

print('Unique Characters:', len(char_set))

chars_sorted = sorted(char_set)
char2int = {ch:i for i, ch in enumerate(chars_sorted) }
char_array = np.array(chars_sorted)

text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)
print("Text encoded shape: ", text_encoded.shape)

print(text[:15], '=== Encoded ==> ', text_encoded[:15])

print(text_encoded[15:35], '=== reverse ==>', ''.join(char_array[text_encoded[15:35]]))

ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)

for example in ds_text_encoded.take(5):
    print('{} --> {}'.format(example.numpy(), char_array[example.numpy()]))

seq_len = 50
chunk_size = seq_len + 1

ds_chunks = ds_text_encoded.batch(batch_size=chunk_size, drop_remainder=True)

def split_input_target(chunk):
    input_seq = chunk[:-1]
    target_seq = chunk[1:]

    return input_seq, target_seq

ds_sequences = ds_chunks.map(split_input_target)

for sample in ds_sequences.take(2):
    print('  Input (x) : ', repr(''.join(char_array[sample[0].numpy()])))
    print('  Target (y)  : ', repr(''.join(char_array[sample[1].numpy()])))
    print()

BATCH_SIZE = 64
BUFFER_SIZE = 10000

ds = ds_sequences.shuffle(BUFFER_SIZE).batch(batch_size=BATCH_SIZE)

"""#### Building a character-level RNN model."""

def build_model(vocab_size, embedding_dim, rnn_units):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim),
        tf.keras.layers.LSTM(rnn_units, return_sequences=True),
        tf.keras.layers.Dense(vocab_size)
    ])

    return model

#Set training parameters.
charset_size = len(char_array)
embedding_dim = 256
rnn_units = 512

tf.random.set_seed(42)

model = build_model(vocab_size=charset_size,
                    embedding_dim=embedding_dim,
                    rnn_units=rnn_units)

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

model.fit(ds, epochs=20)

"""#### Evaluation phase â€“ generating new text passages."""

tf.random.set_seed(42)

logits = [[1.0, 1.0, 1.0]]
print('Probabilities: ', tf.math.softmax(logits).numpy()[0])

samples = tf.random.categorical(logits=logits, num_samples=10)
tf.print(samples.numpy())

def sample(model, starting_str,
           len_generated_txt=700,
           max_input_length=50, scale_factor=1.0):
    encoded_input = [char2int[s] for s in starting_str]
    encoded_input = tf.reshape(encoded_input, (1, -1))

    generated_str = starting_str
    model.reset_states()
    for i in range(len_generated_txt):
        logits = model(encoded_input)
        logits = tf.squeeze(logits, 0)

        scaled_logits = logits * scale_factor
        new_char_idx = tf.random.categorical(scaled_logits, num_samples=1)
        new_char_idx = tf.squeeze(new_char_idx)[-1].numpy()

        generated_str += str(char_array[new_char_idx])
        new_char_idx = tf.expand_dims([new_char_idx], 0)
        encoded_input = tf.concat([encoded_input, new_char_idx], axis=1)
        encoded_input = encoded_input[:, -max_input_length:]

    return generated_str

tf.random.set_seed(1)
print(sample(model, starting_str='Machine Learning'))

model.save('Island.keras')

loaded = tf.keras.models.load_model('Island.keras')

print(sample(loaded, starting_str='This life is not', len_generated_txt=5000))

print(sample(loaded, starting_str='Ngwendete muno', len_generated_txt=500, scale_factor=2.0))

print(sample(loaded, starting_str='Ngwendete muno', len_generated_txt=500, scale_factor=0.5))

print(sample(loaded, starting_str='Ngwendete muno', len_generated_txt=500, scale_factor=5.0))

print(sample(loaded, starting_str='The island', len_generated_txt=1000, scale_factor=1.5))

